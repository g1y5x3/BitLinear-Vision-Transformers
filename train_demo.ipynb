{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99a416-4540-4234-ae67-6721b5e6d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import SequentialSampler, BatchSampler, DataLoader\n",
    "\n",
    "import datasets.transforms as T\n",
    "from datasets.coco import CocoDetection, collate_fn\n",
    "from models.backbone import ResNetBackbone\n",
    "from models.transformer import TransformerBitLinear\n",
    "from models.detr import DETR, SetCriterion\n",
    "from models.matcher import HungarianMatcher\n",
    "from util.misc import rescale_bboxes, plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep random seed fixed to keep my sanity\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "class Args:\n",
    "  coco_path = \"/workspace/coco\"\n",
    "  dataset_file = \"coco\"\n",
    "  masks = False\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "transform_train = T.Compose([\n",
    "  # augumentation\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.RandomSelect(\n",
    "    T.RandomResize(scales, max_size=1333),\n",
    "    T.Compose([\n",
    "      T.RandomResize([400, 500, 600]),\n",
    "      T.RandomSizeCrop(384, 600),\n",
    "      T.RandomResize(scales, max_size=1333),\n",
    "    ])\n",
    "  ),\n",
    "  # normalize\n",
    "  T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "  ])\n",
    "])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CocoDetection(\"/workspace/coco/train2017\", \"/workspace/coco/annotations/instances_train2017.json\", transform_train, return_masks=False)\n",
    "sampler_train = SequentialSampler(dataset)\n",
    "batch_sampler_train = BatchSampler(sampler_train, batch_size=8, drop_last=True)\n",
    "data_loader_train = DataLoader(dataset, batch_sampler=batch_sampler_train, collate_fn=collate_fn, num_workers=1)\n",
    "batch_fetcher = iter(data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad378c-9c03-43d8-95c1-394a0c971004",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_mask, y = next(batch_fetcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2375620-2f27-4b40-844a-31fd8f91053e",
   "metadata": {},
   "source": [
    "## DETR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d18675",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "backbone = ResNetBackbone()\n",
    "transformer = TransformerBitLinear(256, 8, 6, 6, 2048, 0.1)\n",
    "model = DETR(backbone, transformer, num_classes=91, num_queries=100).to(device)\n",
    "\n",
    "matcher = HungarianMatcher()\n",
    "criterion = SetCriterion(91, matcher).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()\n",
    "x, x_mask = x.to(device), x_mask.to(device) \n",
    "y = [{k: v.to(device) for k, v in t.items()} for t in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439cab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epochs in range(500):\n",
    "  optimizer.zero_grad()\n",
    "  with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "    outputs_logits, outputs_boxes = model(x, x_mask)\n",
    "    loss = criterion(outputs_logits, outputs_boxes, y)\n",
    "\n",
    "  writer.add_scalar(\"Loss/train\", loss, epochs)\n",
    " \n",
    "  scaler.scale(loss).backward()\n",
    "  torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "  scaler.step(optimizer)\n",
    "  scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303424b8",
   "metadata": {},
   "source": [
    "## Inspect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "with torch.no_grad():\n",
    "  h, w = y[i][\"size\"]\n",
    "  im = x[i].cpu().numpy().transpose((1,2,0))\n",
    "  prob = outputs_logits[i,:].softmax(-1)[:, :-1]\n",
    "  keep = prob.max(-1).values > 0.25\n",
    "  plot_results(im, prob[keep], rescale_bboxes(outputs_boxes[i][keep,:], (w,h)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b517c5",
   "metadata": {},
   "source": [
    "## Inspect ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c49585",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "h, w = y[i][\"size\"]\n",
    "im = x[i].cpu().numpy().transpose((1,2,0))\n",
    "prob = torch.zeros((len(y[i][\"labels\"]), 91))\n",
    "prob[torch.arange(len(y[i][\"labels\"])), y[i][\"labels\"].cpu()] = 1\n",
    "plot_results(im, prob, rescale_bboxes(y[i][\"boxes\"], (w,h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda49dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from torchvision import models, datasets, tv_tensors\n",
    "from torchvision.transforms import v2\n",
    "from helpers import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c83279",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# create dataset\n",
    "\tscales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "\ttransform_train = T.Compose([\n",
    "\t  # augumentation\n",
    "\t  T.RandomHorizontalFlip(),\n",
    "\t  T.RandomSelect(\n",
    "\t    T.RandomResize(scales, max_size=1333),\n",
    "\t    T.Compose([\n",
    "\t      T.RandomResize([400, 500, 600]),\n",
    "\t      T.RandomSizeCrop(384, 600),\n",
    "\t      T.RandomResize(scales, max_size=1333),\n",
    "\t    ])\n",
    "\t  ),\n",
    "\t  # normalize\n",
    "\t  T.Compose([\n",
    "\t    T.ToTensor(),\n",
    "\t    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t  ])\n",
    "\t])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeBoundingBox(object):\n",
    "  def __call__(self, images, targets):\n",
    "    print(images)\n",
    "    print(targets)\n",
    "    return images, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d07c4bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=12.52s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "transforms = v2.Compose(\n",
    "  [\n",
    "    v2.ToImage(),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomShortestSize(min_size=(480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800), max_size=1333),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    v2.ConvertBoundingBoxFormat(\"CXCYWH\"),\n",
    "    v2.SanitizeBoundingBoxes(),\n",
    "    NormalizeBoundingBox(),\n",
    "    v2.ToPureTensor(),\n",
    "  ]\n",
    ")\n",
    "\n",
    "dataset = datasets.CocoDetection(\"/workspace/coco/train2017\", \"/workspace/coco/annotations/instances_train2017.json\", transforms=transforms)\n",
    "dataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=(\"boxes\", \"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "663dd693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(img) = <class 'torch.Tensor'>\n",
      "type(target) = <class 'dict'>\n",
      "target.keys() = dict_keys(['boxes', 'labels'])\n",
      "type(target['boxes']) = <class 'torch.Tensor'>\n",
      "type(target['labels']) = <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "img, target = dataset[0]\n",
    "print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
    "print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71020088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boxes': tensor([[488.2363, 484.8947, 896.3616, 419.2320],\n",
       "         [247.1483, 174.0200, 467.9448, 335.3973],\n",
       "         [340.4354, 515.9880, 463.4892, 359.4507],\n",
       "         [331.2679,  93.3167, 110.7280,  68.2440],\n",
       "         [212.7868,  91.3807,  85.1088,  68.4493],\n",
       "         [311.1375, 159.7420, 123.1418, 103.4147],\n",
       "         [334.9979,  55.7700, 138.8826, 104.2360]]),\n",
       " 'labels': tensor([51, 51, 56, 55, 55, 55, 55])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07b69788",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Boxes need to be in (xmin, ymin, xmax, ymax) format. Use torchvision.ops.box_convert to convert them",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/BitLinear-Vision-Transformers/helpers.py:38\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(imgs, row_title, **imshow_kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mto_dtype(img, torch\u001b[38;5;241m.\u001b[39muint8, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m   img \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myellow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   img \u001b[38;5;241m=\u001b[39m draw_segmentation_masks(img, masks\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool), colors\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m masks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.65\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/utils.py:202\u001b[0m, in \u001b[0;36mdraw_bounding_boxes\u001b[0;34m(image, boxes, labels, colors, fill, width, font, font_size)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly grayscale and RGB images are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (boxes[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m boxes[:, \u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m (boxes[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m boxes[:, \u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoxes need to be in (xmin, ymin, xmax, ymax) format. Use torchvision.ops.box_convert to convert them\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    206\u001b[0m num_boxes \u001b[38;5;241m=\u001b[39m boxes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_boxes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Boxes need to be in (xmin, ymin, xmax, ymax) format. Use torchvision.ops.box_convert to convert them"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdU0lEQVR4nO3df2zddb348Vfb0VOItMy7u+6HxWUYRAU23VgtSAimugQz7v4w7oLZ5sKPi+4SXaOyOVhFdJ2IuASGixPEP/RuSsAYtwy1shhkZsm2JngZEBi4aWxh0bVzSMva9/3DL/XbrYWd7vTHe3s8kvNHP3w+Pa8C55Vnz2l7ylJKKQAAMlA+1gMAAJws4QIAZEO4AADZEC4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBko+hw+e1vfxsLFiyIadOmRVlZWfzsZz9722t27NgRH/rQh6JQKMR73vOeePjhh4cxKpArewMolaLD5ejRozFr1qzYsGHDSZ3/0ksvxSc+8Ym4+uqro62tLb7whS/EjTfeGI8//njRwwJ5sjeAUik7lTdZLCsri8ceeywWLlw45Dm33XZbbN26Nf7whz/0H/vP//zPOHz4cGzfvn24dw1kyt4ATsWEkb6DnTt3RmNj44Bj8+fPjy984QtDXtPd3R3d3d39H/f19cVf//rX+Ld/+7coKysbqVGBIaSU4siRIzFt2rQoLx/5H42zN+D0MBK7Y8TDpb29PWprawccq62tja6urvjHP/4RZ5999gnXtLS0xJ133jnSowFFOnjwYLzrXe8a8fuxN+D0UsrdMeLhMhyrVq2Kpqam/o87Ozvj/PPPj4MHD0Z1dfUYTgZnpq6urqirq4tzzz13rEcZkr0B489I7I4RD5cpU6ZER0fHgGMdHR1RXV096HdNERGFQiEKhcIJx6urqy0gGEOj9ZKLvQGnl1LujhF/sbqhoSFaW1sHHPvVr34VDQ0NI33XQKbsDWAoRYfL3//+92hra4u2traI+OevLba1tcWBAwci4p9P1y5ZsqT//FtuuSX2798fX/7yl+PZZ5+NBx54IH7yk5/EihUrSvMVAOOevQGUTCrSE088kSLihNvSpUtTSiktXbo0XXXVVSdcM3v27FRZWZlmzpyZfvCDHxR1n52dnSkiUmdnZ7HjAiVwqo9BewPOTCPxODylv+MyWrq6uqKmpiY6Ozu9Vg1jIMfHYI4zw+lmJB6H3qsIAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsDCtcNmzYEDNmzIiqqqqor6+PXbt2veX569evj/e+971x9tlnR11dXaxYsSJef/31YQ0M5MneAEqh6HDZsmVLNDU1RXNzc+zZsydmzZoV8+fPj1deeWXQ83/84x/HypUro7m5Ofbt2xcPPvhgbNmyJb7yla+c8vBAHuwNoFSKDpd77703brrppli2bFm8//3vj40bN8Y555wTDz300KDnP/XUU3HFFVfE9ddfHzNmzIiPf/zjcd11173td1vA6cPeAEqlqHDp6emJ3bt3R2Nj478+QXl5NDY2xs6dOwe95vLLL4/du3f3L5z9+/fHtm3b4pprrhnyfrq7u6Orq2vADciTvQGU0oRiTj506FD09vZGbW3tgOO1tbXx7LPPDnrN9ddfH4cOHYqPfOQjkVKKY8eOxS233PKWT/m2tLTEnXfeWcxowDhlbwClNOK/VbRjx45Yu3ZtPPDAA7Fnz5549NFHY+vWrXHXXXcNec2qVauis7Oz/3bw4MGRHhMYR+wNYChFPeMyadKkqKioiI6OjgHHOzo6YsqUKYNec8cdd8TixYvjxhtvjIiISy65JI4ePRo333xzrF69OsrLT2ynQqEQhUKhmNGAccreAEqpqGdcKisrY86cOdHa2tp/rK+vL1pbW6OhoWHQa1577bUTlkxFRUVERKSUip0XyIy9AZRSUc+4REQ0NTXF0qVLY+7cuTFv3rxYv359HD16NJYtWxYREUuWLInp06dHS0tLREQsWLAg7r333vjgBz8Y9fX18cILL8Qdd9wRCxYs6F9EwOnN3gBKpehwWbRoUbz66quxZs2aaG9vj9mzZ8f27dv7f/DuwIEDA75Tuv3226OsrCxuv/32+POf/xz//u//HgsWLIhvfOMbpfsqgHHN3gBKpSxl8LxrV1dX1NTURGdnZ1RXV4/1OHDGyfExmOPMcLoZiceh9yoCALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwwqXDRs2xIwZM6Kqqirq6+tj165db3n+4cOHY/ny5TF16tQoFApx4YUXxrZt24Y1MJAnewMohQnFXrBly5ZoamqKjRs3Rn19faxfvz7mz58fzz33XEyePPmE83t6euJjH/tYTJ48OR555JGYPn16/PGPf4zzzjuvFPMDGbA3gFIpSymlYi6or6+Pyy67LO6///6IiOjr64u6urq49dZbY+XKlSecv3HjxvjWt74Vzz77bJx11lnDGrKrqytqamqis7Mzqqurh/U5gOE71cegvQFnppF4HBb1UlFPT0/s3r07Ghsb//UJysujsbExdu7cOeg1P//5z6OhoSGWL18etbW1cfHFF8fatWujt7d3yPvp7u6Orq6uATcgT/YGUEpFhcuhQ4eit7c3amtrBxyvra2N9vb2Qa/Zv39/PPLII9Hb2xvbtm2LO+64I7797W/H17/+9SHvp6WlJWpqavpvdXV1xYwJjCP2BlBKI/5bRX19fTF58uT43ve+F3PmzIlFixbF6tWrY+PGjUNes2rVqujs7Oy/HTx4cKTHBMYRewMYSlE/nDtp0qSoqKiIjo6OAcc7OjpiypQpg14zderUOOuss6KioqL/2Pve975ob2+Pnp6eqKysPOGaQqEQhUKhmNGAccreAEqpqGdcKisrY86cOdHa2tp/rK+vL1pbW6OhoWHQa6644op44YUXoq+vr//Y888/H1OnTh10+QCnF3sDKKWiXypqamqKTZs2xQ9/+MPYt29ffPazn42jR4/GsmXLIiJiyZIlsWrVqv7zP/vZz8Zf//rX+PznPx/PP/98bN26NdauXRvLly8v3VcBjGv2BlAqRf8dl0WLFsWrr74aa9asifb29pg9e3Zs3769/wfvDhw4EOXl/+qhurq6ePzxx2PFihVx6aWXxvTp0+Pzn/983HbbbaX7KoBxzd4ASqXov+MyFvw9BhhbOT4Gc5wZTjdj/ndcAADGknABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAwrXDZs2BAzZsyIqqqqqK+vj127dp3UdZs3b46ysrJYuHDhcO4WyJzdAZyqosNly5Yt0dTUFM3NzbFnz56YNWtWzJ8/P1555ZW3vO7ll1+OL37xi3HllVcOe1ggX3YHUApFh8u9994bN910Uyxbtize//73x8aNG+Occ86Jhx56aMhrent749Of/nTceeedMXPmzLe9j+7u7ujq6hpwA/I20rvD3oAzQ1Hh0tPTE7t3747GxsZ/fYLy8mhsbIydO3cOed3Xvva1mDx5ctxwww0ndT8tLS1RU1PTf6urqytmTGCcGY3dYW/AmaGocDl06FD09vZGbW3tgOO1tbXR3t4+6DVPPvlkPPjgg7Fp06aTvp9Vq1ZFZ2dn/+3gwYPFjAmMM6OxO+wNODNMGMlPfuTIkVi8eHFs2rQpJk2adNLXFQqFKBQKIzgZMJ4NZ3fYG3BmKCpcJk2aFBUVFdHR0THgeEdHR0yZMuWE81988cV4+eWXY8GCBf3H+vr6/nnHEybEc889FxdccMFw5gYyYncApVLUS0WVlZUxZ86caG1t7T/W19cXra2t0dDQcML5F110UTz99NPR1tbWf7v22mvj6quvjra2Nq9BwxnC7gBKpeiXipqammLp0qUxd+7cmDdvXqxfvz6OHj0ay5Yti4iIJUuWxPTp06OlpSWqqqri4osvHnD9eeedFxFxwnHg9GZ3AKVQdLgsWrQoXn311VizZk20t7fH7NmzY/v27f0/dHfgwIEoL/cHeYGB7A6gFMpSSmmsh3g7XV1dUVNTE52dnVFdXT3W48AZJ8fHYI4zw+lmJB6Hvr0BALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwgUAyIZwAQCyIVwAgGwIFwAgG8IFAMiGcAEAsiFcAIBsCBcAIBvCBQDIhnABALIhXACAbAgXACAbwwqXDRs2xIwZM6Kqqirq6+tj165dQ567adOmuPLKK2PixIkxceLEaGxsfMvzgdOX3QGcqqLDZcuWLdHU1BTNzc2xZ8+emDVrVsyfPz9eeeWVQc/fsWNHXHfddfHEE0/Ezp07o66uLj7+8Y/Hn//851MeHsiH3QGUQllKKRVzQX19fVx22WVx//33R0REX19f1NXVxa233horV6582+t7e3tj4sSJcf/998eSJUsGPae7uzu6u7v7P+7q6oq6urro7OyM6urqYsYFSqCrqytqampO6TE40rvD3oDxpxS743hFPePS09MTu3fvjsbGxn99gvLyaGxsjJ07d57U53jttdfijTfeiHe+851DntPS0hI1NTX9t7q6umLGBMaZ0dgd9gacGYoKl0OHDkVvb2/U1tYOOF5bWxvt7e0n9Tluu+22mDZt2oAFdrxVq1ZFZ2dn/+3gwYPFjAmMM6OxO+wNODNMGM07W7duXWzevDl27NgRVVVVQ55XKBSiUCiM4mTAeHYyu8PegDNDUeEyadKkqKioiI6OjgHHOzo6YsqUKW957T333BPr1q2LX//613HppZcWPymQLbsDKJWiXiqqrKyMOXPmRGtra/+xvr6+aG1tjYaGhiGvu/vuu+Ouu+6K7du3x9y5c4c/LZAluwMolaJfKmpqaoqlS5fG3LlzY968ebF+/fo4evRoLFu2LCIilixZEtOnT4+WlpaIiPjmN78Za9asiR//+McxY8aM/tez3/GOd8Q73vGOEn4pwHhmdwClUHS4LFq0KF599dVYs2ZNtLe3x+zZs2P79u39P3R34MCBKC//1xM53/3ud6Onpyc++clPDvg8zc3N8dWvfvXUpgeyYXcApVD033EZCyPxe+DAycvxMZjjzHC6GfO/4wIAMJaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJAN4QIAZEO4AADZEC4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJAN4QIAZEO4AADZEC4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJAN4QIAZEO4AADZEC4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJAN4QIAZEO4AADZEC4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJAN4QIAZEO4AADZEC4AQDaECwCQDeECAGRjWOGyYcOGmDFjRlRVVUV9fX3s2rXrLc//6U9/GhdddFFUVVXFJZdcEtu2bRvWsEDe7A7gVBUdLlu2bImmpqZobm6OPXv2xKxZs2L+/PnxyiuvDHr+U089Fdddd13ccMMNsXfv3li4cGEsXLgw/vCHP5zy8EA+7A6gFMpSSqmYC+rr6+Oyyy6L+++/PyIi+vr6oq6uLm699dZYuXLlCecvWrQojh49Gr/4xS/6j334wx+O2bNnx8aNGwe9j+7u7uju7u7/uLOzM84///w4ePBgVFdXFzMuUAJdXV1RV1cXhw8fjpqammF9jpHeHfYGjD+l2B0nSEXo7u5OFRUV6bHHHhtwfMmSJenaa68d9Jq6urr0ne98Z8CxNWvWpEsvvXTI+2lubk4R4ebmNs5uL774YjErY1R3h73h5jZ+b8PdHYOZEEU4dOhQ9Pb2Rm1t7YDjtbW18eyzzw56TXt7+6Dnt7e3D3k/q1atiqampv6PDx8+HO9+97vjwIEDpSu2EfZmZeb03Z6ZR0eOM7/57MU73/nOYV0/GrvD3hgbOc4ckefcOc58qrtjMEWFy2gpFApRKBROOF5TU5PNf6w3VVdXm3kUmHl0lJeP319EtDfGVo4zR+Q5d44zl3J3FPWZJk2aFBUVFdHR0THgeEdHR0yZMmXQa6ZMmVLU+cDpx+4ASqWocKmsrIw5c+ZEa2tr/7G+vr5obW2NhoaGQa9paGgYcH5ExK9+9ashzwdOP3YHUDLF/lDM5s2bU6FQSA8//HB65pln0s0335zOO++81N7enlJKafHixWnlypX95//ud79LEyZMSPfcc0/at29fam5uTmeddVZ6+umnT/o+X3/99dTc3Jxef/31YscdM2YeHWYeHaWYebR3x5n673m05ThzSnnObeZ/KjpcUkrpvvvuS+eff36qrKxM8+bNS7///e/7/9lVV12Vli5dOuD8n/zkJ+nCCy9MlZWV6QMf+EDaunXrKQ0N5MnuAE5V0X/HBQBgrIzfXxEAADiOcAEAsiFcAIBsCBcAIBvjJlxyfLv7YmbetGlTXHnllTFx4sSYOHFiNDY2vu3XOBKK/ff8ps2bN0dZWVksXLhwZAccRLEzHz58OJYvXx5Tp06NQqEQF1544aj//1HszOvXr4/3vve9cfbZZ0ddXV2sWLEiXn/99VGaNuK3v/1tLFiwIKZNmxZlZWXxs5/97G2v2bFjR3zoQx+KQqEQ73nPe+Lhhx8e8TmPZ2+MDntj9OS0O8Zsb4z1rzWl9M+/71BZWZkeeuih9L//+7/ppptuSuedd17q6OgY9Pzf/e53qaKiIt19993pmWeeSbfffnvRfxtmtGe+/vrr04YNG9LevXvTvn370mc+85lUU1OT/vSnP43bmd/00ksvpenTp6crr7wy/cd//MfoDPv/FDtzd3d3mjt3brrmmmvSk08+mV566aW0Y8eO1NbWNm5n/tGPfpQKhUL60Y9+lF566aX0+OOPp6lTp6YVK1aM2szbtm1Lq1evTo8++miKiBPeDPF4+/fvT+ecc05qampKzzzzTLrvvvtSRUVF2r59++gMnOyN8Trzm+yNkZ97rHfHWO2NcREu8+bNS8uXL+//uLe3N02bNi21tLQMev6nPvWp9IlPfGLAsfr6+vRf//VfIzrn/6/YmY937NixdO6556Yf/vCHIzXiCYYz87Fjx9Lll1+evv/976elS5eO+gIqdubvfve7aebMmamnp2e0RjxBsTMvX748ffSjHx1wrKmpKV1xxRUjOudQTmYBffnLX04f+MAHBhxbtGhRmj9//ghONpC9MTrsjdGT8+4Yzb0x5i8V9fT0xO7du6OxsbH/WHl5eTQ2NsbOnTsHvWbnzp0Dzo+ImD9//pDnl9pwZj7ea6+9Fm+88UZJ3zHzrQx35q997WsxefLkuOGGG0ZjzAGGM/PPf/7zaGhoiOXLl0dtbW1cfPHFsXbt2ujt7R23M19++eWxe/fu/qeE9+/fH9u2bYtrrrlmVGYejhwfgznOfDx74+3luDcizozdUarH4Ji/O/RovN19qQ1n5uPddtttMW3atBP+I46U4cz85JNPxoMPPhhtbW2jMOGJhjPz/v374ze/+U18+tOfjm3btsULL7wQn/vc5+KNN96I5ubmcTnz9ddfH4cOHYqPfOQjkVKKY8eOxS233BJf+cpXRnze4RrqMdjV1RX/+Mc/4uyzzx7R+7c37I2h5Lg3Is6M3VGqvTHmz7icidatWxebN2+Oxx57LKqqqsZ6nEEdOXIkFi9eHJs2bYpJkyaN9Tgnra+vLyZPnhzf+973Ys6cObFo0aJYvXp1bNy4caxHG9KOHTti7dq18cADD8SePXvi0Ucfja1bt8Zdd9011qMxjtgbIyfHvRFx5u6OMX/GJce3ux/OzG+65557Yt26dfHrX/86Lr300pEcc4BiZ37xxRfj5ZdfjgULFvQf6+vri4iICRMmxHPPPRcXXHDBuJo5ImLq1Klx1llnRUVFRf+x973vfdHe3h49PT1RWVk57ma+4447YvHixXHjjTdGRMQll1wSR48ejZtvvjlWr14d5eXj7/uLoR6D1dXVI/5sS4S9MVrsjdHZGxFnxu4o1d4Y868qx7e7H87MERF333133HXXXbF9+/aYO3fuaIzar9iZL7roonj66aejra2t/3bttdfG1VdfHW1tbVFXVzfuZo6IuOKKK+KFF17oX5YREc8//3xMnTp1VJbPcGZ+7bXXTlgwby7QNE7fSizHx2COM0fYGyM9c8TY742IM2N3lOwxWNSP8o6Q0X67+7GYed26damysjI98sgj6S9/+Uv/7ciRI+N25uONxW8HFDvzgQMH0rnnnpv++7//Oz333HPpF7/4RZo8eXL6+te/Pm5nbm5uTueee276n//5n7R///70y1/+Ml1wwQXpU5/61KjNfOTIkbR37960d+/eFBHp3nvvTXv37k1//OMfU0oprVy5Mi1evLj//Dd/rfFLX/pS2rdvX9qwYcOY/Dq0vTH+Zj6evTFyc4/17hirvTEuwiWlPN/uvpiZ3/3ud6eIOOHW3Nw8bmc+3lgsoJSKn/mpp55K9fX1qVAopJkzZ6ZvfOMb6dixY+N25jfeeCN99atfTRdccEGqqqpKdXV16XOf+1z629/+NmrzPvHEE4P+//nmnEuXLk1XXXXVCdfMnj07VVZWppkzZ6Yf/OAHozbvm+yN8Tfz8eyN4uS0O8Zqb5SlNA6fTwIAGMSY/4wLAMDJEi4AQDaECwCQDeECAGRDuAAA2RAuAEA2hAsAkA3hAgBkQ7gAANkQLgBANoQLAJCN/wM67QKIu94xvgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot([dataset[0], dataset[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
